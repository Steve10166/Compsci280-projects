<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CS280 Projects — Project 0 & 1</title>
  <style>
    :root{
      --bg:#f9f9f9; --text:#333; --muted:#666;
      --brand:#16a085; --brandDark:#0f7c67; --head:#2c3e50;
      --card:#fff; --shadow:0 2px 8px rgba(0,0,0,.12);
    }
    *{box-sizing:border-box}
    body{font-family:Arial,Helvetica,sans-serif;background:var(--bg);color:var(--text);margin:0}
    header{padding:28px 20px 10px}
    .wrap{max-width:1100px;margin:0 auto;padding:0 20px 40px}
    h1{color:var(--head);margin:0 0 6px}
    .sub{color:var(--muted);margin:0 0 18px}

    /* Tabs */
    .tabs{display:flex;gap:6px;border-bottom:1px solid #e6e6e6;margin:18px 0 28px}
    .tab{
      appearance:none;border:none;background:none;
      padding:10px 14px;border-radius:10px 10px 0 0;cursor:pointer;
      font-weight:600;color:var(--head)
    }
    .tab.active{background:var(--card);box-shadow:var(--shadow)}
    section[role="tabpanel"]{display:none;background:var(--card);box-shadow:var(--shadow);border-radius:14px;padding:22px}
    section[role="tabpanel"].active{display:block}

    /* Common */
    h2{color:var(--brand);margin:8px 0 12px}
    h3{color:var(--head);margin:22px 0 8px}
    p{line-height:1.65}
    details{background:#fafafa;border:1px solid #eee;border-radius:10px;padding:12px 14px}
    details>summary{font-weight:600;color:var(--head);cursor:pointer;margin:-4px 0 8px}
    .grid{
      display:grid;gap:16px;grid-template-columns:repeat(auto-fit,minmax(220px,1fr));
      margin-top:10px
    }
    figure{margin:0;background:#fff;border:1px solid #eee;border-radius:10px;box-shadow:var(--shadow);overflow:hidden}
    figure img{width:100%;height:auto;display:block}
    figcaption{padding:8px 10px;font-size:.92rem;color:var(--muted)}
    .hero-links{display:grid;grid-template-columns:repeat(auto-fit,minmax(240px,1fr));gap:12px;margin-top:8px}
    .cardlink{display:block;background:var(--card);padding:16px 18px;border-radius:14px;text-decoration:none;color:var(--head);box-shadow:var(--shadow);border:1px solid #eee}
    .cardlink:hover{transform:translateY(-1px);box-shadow:0 6px 14px rgba(0,0,0,.12)}
    .note{font-size:.92rem;color:var(--muted)}
    table{width:100%;border-collapse:collapse;margin-top:8px}
    th,td{border:1px solid #e6e6e6;padding:8px 10px;text-align:left;font-size:.95rem}
    th{background:#fbfbfb}
    code.kbd{background:#f1f1f1;border:1px solid #e5e5e5;padding:2px 6px;border-radius:6px}
  </style>
</head>
<body>
  <header class="wrap">
    <h1>CS280 Projects</h1>
    <p class="sub">Project 0: Camera basics · Project 1: Prokudin-Gorskii colorization & alignment · Project 2: Fun with Filters and Frequencies!</p>
    <div class="hero-links">
      <a class="cardlink" href="#p0" onclick="switchTab('p0')">Go to Project 0</a>
      <a class="cardlink" href="#p1" onclick="switchTab('p1')">Go to Project 1</a>
      <a class="cardlink" href="#p2" onclick="switchTab('p2')">Go to Project 2</a>
    </div>
  </header>

  <div class="wrap">
    <div class="tabs" role="tablist" aria-label="Projects">
      <button id="tab-p0" class="tab active" role="tab" aria-controls="p0" aria-selected="true" onclick="switchTab('p0')">
        Project 0
      </button>
      <button id="tab-p1" class="tab" role="tab" aria-controls="p1" aria-selected="false" onclick="switchTab('p1')">
        Project 1
      </button>
      <button id="tab-p2" class="tab" role="tab" aria-controls="p2" aria-selected="false" onclick="switchTab('p2')">
        Project 2
      </button>
    </div>

    <!-- ========== PROJECT 0 (unchanged content from your file) ========== -->
    <section id="p0" role="tabpanel" aria-labelledby="tab-p0" class="active">
      <h2>Project 0: Becoming Friends with Your Camera</h2>

      <h3>Part 1: Selfie — The Wrong Way vs. The Right Way</h3>
      <p>
        Take a picture of your friend (or yourself) from close up. You get a typical distorted selfie image.
        Now step back several feet from your subject, zoom in, and take a second picture. Try to get the face in the
        second photo to be the same size as in the first photo. If you've done things right, the second portrait should
        look much better than the first one. Think about why this is.
      </p>
      <div class="grid">
        <figure>
          <img src="project0/close_selfie.jpg" alt="Close-up selfie">
          <figcaption>The Wrong Way: Close-up Selfie</figcaption>
        </figure>
        <figure>
          <img src="project0/far_selfie.jpg" alt="Far away selfie">
          <figcaption>The Right Way: Step Back &amp; Zoom In</figcaption>
        </figure>
      </div>
      <p class="note">
        Close distance exaggerates perspective (large nose, stretched features). Stepping back and zooming in restores proportions.
      </p>

      <h3>Part 2: Architectural Perspective Compression</h3>
      <p>
        Repeat the procedure in reverse for an urban scene. The telephoto view appears flattened relative to the wide angle,
        due to perspective compression when shooting from farther away.
      </p>
      <div class="grid">
        <figure>
          <img src="project0/close_building.jpg" alt="Zoomed-in urban scene">
          <figcaption>Compressed View: Zoomed In</figcaption>
        </figure>
        <figure>
          <img src="project0/far_building.jpg" alt="Wide urban scene">
          <figcaption>Expanded View: Close with Wide Angle</figcaption>
        </figure>
      </div>

      <h3>Part 3: The Dolly Zoom</h3>
      <p>The classic “Vertigo shot” keeps subject size constant while changing focal length and distance.</p>
      <figure style="max-width:420px">
        <img src="project0/hitchcock.gif" alt="Dolly zoom effect">
        <figcaption>Example of the Dolly Zoom (Vertigo Shot)</figcaption>
      </figure>
    </section>

    <section id="p1" role="tabpanel" aria-labelledby="tab-p1">
      <h2>Project 1: Prokudin-Gorskii Colorization & Alignment</h2>

      <details open>
        <summary>Overview &amp; Approach</summary>
        <p>
          I split each glass-plate image vertically into three equal bands (top→bottom = B, G, R). The goal is to align
          the G and R bands to the B reference and then stack into an RGB image. I implemented:
        </p>
        <ul>
          <li><b>Pyramid alignment</b>: I align each channel to blue using a coarse-to-fine strategy. At scales <code>[0.125, 0.25, 0.5, 1.0]</code> I rescale both the reference and moving images, apply the cumulative shift so far, and estimate a new offset with <code>phase_cross_correlation(..., upsample_factor=20)</code> for sub-pixel accuracy. I accumulate these scale-normalized deltas to get the final <code>[dy, dx]</code> for G→B and R→B.</li>
        
          <li><b>Automatic border trimming</b>: After a conservative fixed crop (<code>margin = 200</code> pixels on all sides), I run a content-aware trim. I convert to grayscale, compute per-pixel saturation, and derive robust thresholds from the central 80% of the image: the 5% and 98% grayscale quantiles for “too dark/bright,” and <code>median(saturation)+0.28</code> for “too saturated.” I mark a rim mask where any of these hold and scan inward along rows/columns with a sliding window (width 22) until the rim proportion falls below 0.60. This returns slice indices so only the clean overlapping region remains—no black/white/colored edges from mis-registration.</li>
        
          <li><b>Contrast stretching &amp; white balance</b>: I stretch intensities with <code>skimage.exposure.rescale_intensity(..., in_range="image")</code> so the current frame uses the full 0–1 range. For color neutrality, I apply “white-patch” balancing: I take the 95th percentile per channel, invert to get per-channel gains, normalize them by the green channel, and scale the image so highlights look neutral instead of tinted.</li>
        
          <li><b>Color correction (RGB mixing)</b>: Because the three historical glass filters don’t map perfectly to modern RGB primaries, I tested various 3×3 correction matrix before balancing:
            <pre style="margin:.5em 0 0;">[[ 1.20, -0.10, -0.10],
         [ -0.05,  1.10, -0.05],
         [ -0.05, -0.10,  1.10]]</pre>
            I multiply the (H×W×3) image by this matrix and clip to [0, 1]. This gently boosts each channel while subtracting small cross-talk from the others, producing more natural color without heavy-handed grading.</li>
        
          <li><b>Data flow &amp; outputs</b>: For each input glass plate (stacked B/G/R vertically), I split into thirds (<code>B = top, G = middle, R = bottom</code>), align G and R to B, perform margin crop → trim → contrast stretch → color correction → white balance.</li>
        </ul>
      </details>

      <h3>Results</h3>
      <p class="note">All images below are my code's result.</p>

      <div class="grid" id="results-grid">
        <!-- Each figure references your existing files -->
        <figure><img src="proj1/results/church.png" alt="church"><figcaption>church.png</figcaption></figure>
        <figure><img src="proj1/results/emir.png" alt="emir"><figcaption>emir.png</figcaption></figure>
        <figure><img src="proj1/results/harvesters.png" alt="harvesters"><figcaption>harvesters.png</figcaption></figure>
        <figure><img src="proj1/results/icon.png" alt="icon"><figcaption>icon.png</figcaption></figure>
        <figure><img src="proj1/results/italil.png" alt="italil"><figcaption>italil.png</figcaption></figure>
        <figure><img src="proj1/results/lastochikino.png" alt="lastochikino"><figcaption>lastochikino.png</figcaption></figure>
        <figure><img src="proj1/results/lugano.png" alt="lugano"><figcaption>lugano.png</figcaption></figure>
        <figure><img src="proj1/results/melons.png" alt="melons"><figcaption>melons.png</figcaption></figure>
        <figure><img src="proj1/results/mosque.png" alt="mosque"><figcaption>mosque.png</figcaption></figure>
        <figure><img src="proj1/results/patheon.png" alt="patheon"><figcaption>patheon.png</figcaption></figure>
        <figure><img src="proj1/results/redwood.png" alt="redwood"><figcaption>redwood.png</figcaption></figure>
        <figure><img src="proj1/results/self_portrait.png" alt="self_portrait"><figcaption>self_portrait.png</figcaption></figure>
        <figure><img src="proj1/results/siren.png" alt="siren"><figcaption>siren.png</figcaption></figure>
        <figure><img src="proj1/results/three_generations.png" alt="three_generations"><figcaption>three_generations.png</figcaption></figure>

      </div>

      <h3>Computed Offsets</h3>

<table>
  <thead>
    <tr>
      <th>Image</th>
      <th>G dx</th><th>G dy</th>
      <th>R dx</th><th>R dy</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>church.png</td><td>3.45</td><td>24.8</td><td>-4.3</td><td>57.80</td><td></td></tr>
    <tr><td>emir.png</td><td>23.70</td><td>48.65</td><td>41.00</td><td>105.95</td><td></td></tr>
    <tr><td>harvesters.png</td><td>13.35</td><td>61.0</td><td>10.25</td><td>123.55</td><td></td></tr>
    <tr><td>icon.png</td><td>16.05</td><td>39.2</td><td>23.10</td><td>88.30</td><td>This looks really realistic.</td></tr>
    <tr><td>italil.png</td><td>21.85</td><td>38.15</td><td>36.05</td><td>76.30</td><td></td></tr>
    <tr><td>lastochikino.png</td><td>-1.70</td><td>-1.95</td><td>-8.90</td><td>76.05</td><td></td></tr>
    <tr><td>lugano.png</td><td>-17.70</td><td>40.50</td><td>-28.80</td><td>92.05</td><td>I really like how this turned out.</td></tr>
    <tr><td>melons.png</td><td>9.70</td><td>79.95</td><td>13.90</td><td>176.20</td><td></td></tr>
    <tr><td>mosque.png</td><td>5.15</td><td>26.75</td><td>10.60</td><td>70.90</td><td></td></tr>
    <tr><td>patheon.png</td><td>27.0</td><td>14.15</td><td>48.95</td><td>85.10</td><td></td></tr>
    <tr><td>redwood.png</td><td>-31.20</td><td>58.30</td><td>-61.40</td><td>131.45</td><td></td></tr>
    <tr><td>self_portrait.png</td><td>24.50</td><td>73.80</td><td>36.85</td><td>174.85</td><td>This one also looked very realistic.</td></tr>
    <tr><td>siren.png</td><td>-5.95</td><td>49.15</td><td>-23.80</td><td>96.15</td><td></td></tr>
    <tr><td>three_generations.png</td><td>11.45</td><td>55.30</td><td>7.25</td><td>106.55</td><td></td></tr>
  </tbody>
</table>
<h3>Before and After: Melons</h3>
<p class="note">The images presented below are before and after bells and whistles were implemented. Take this image as an example, the imaged before only had a fixed trimmer where as the latter image was trimmed with pixel-level edge recognition. The latter image was also equipped with automatic contrasting by setting the darkest pixel to 0 and brightest to 1. In addition, I attempted to apply some level of RGB correction by applying a RGB matrix that makes the image look more natural by decreasing green bleedthrough.</p>
<div style="display: flex; gap: 20px; flex-wrap: wrap;">
  <figure style="text-align:center;">
    <img src="proj1/out_improved.jpg" alt="Melons before" width="400">
    <figcaption>Before (melons.png)</figcaption>
  </figure>
  <figure style="text-align:center;">
    <img src="proj1/results/melons.png" alt="Melons improved" width="400">
    <figcaption>After (out_improved.jpg)</figcaption>
  </figure>
</div>
      
    </section>
    <section id="p2" role="tabpanel" aria-labelledby="tab-p2">
      <h2>Project 2: Convolutions, Gradients, and DoG</h2>
    
      <details open>
        <summary>Overview</summary>
        <p>
          In this project I implement 2D convolution from scratch (four loops → two loops), verify against
          <code>scipy.signal.convolve2d</code>, visualize finite differences and gradient magnitudes, and then denoise edges
          using a Gaussian blur and Derivative-of-Gaussian (DoG) filters. Finally, I compare visual quality before/after
          smoothing and note performance differences between my DIY convolution and SciPy.
        </p>
      </details>
    
      <h3>Part 1.1: Convolutions from Scratch</h3>
      <p>
        I wrote two versions of convolution (zero padding): a naïve 4-loop and a 2-loop (using a sliding window + dot product).
        I compared with <code>scipy.signal.convolve2d</code> on a 9&times;9 box filter applied to my grayscale selfie.
        SciPy is much faster than my DIY implementation for the same kernel and image sizes.
      </p>
    
      <pre><code class="language-python">import numpy as np
    import cv2
    import matplotlib.pyplot as plt
    from scipy.signal import convolve2d
    
    def convolve(img, k):
        img = np.asarray(img, dtype=float)
        k = np.asarray(k, dtype=float)
        kx, ky = k.shape
        ih, iw = img.shape
        px, py = kx//2, ky//2
        pad = np.pad(img, ((px, px), (py, py)))
        out = np.zeros_like(img, dtype=float)
        for i in range(ih):
            for j in range(iw):
                s = 0.0
                for m in range(kx):
                    for n in range(ky):
                        s += pad[i+m, j+n] * k[kx-1-m, ky-1-n]
                out[i, j] = s
        return out
    
    def convolve_2loops(img, k):
        img = np.asarray(img, dtype=float)
        k = np.asarray(k, dtype=float)
        kx, ky = k.shape
        ih, iw = img.shape
        px, py = kx//2, ky//2
        pad = np.pad(img, ((px, px), (py, py)))
        out = np.zeros_like(img, dtype=float)
        for i in range(ih):
            for j in range(iw):
                patch = pad[i:i+kx, j:j+ky]
                out[i, j] = np.sum(patch * k[::-1, ::-1])
        return out
  
    </code></pre>
    
      <div class="grid" style="grid-template-columns:repeat(auto-fit,minmax(160px,1fr));">
        <figure><img src="proj2/my_face.jpg" alt="Selfie" style="max-width:180px"><figcaption>Input</figcaption></figure>
        <figure><img src="proj2/results/box_convolve.png" alt="Box conv" style="max-width:180px"><figcaption>DIY 4-loop Convolution</figcaption></figure>
        <figure><img src="proj2/results/box_convolve_scipy.png" alt="Box conv SciPy" style="max-width:180px"><figcaption>SciPy box filter (much faster!)</figcaption></figure>
      </div>
    
      <h3>Part 1.2: Finite Difference Operator</h3>
      <p>
        I convolve the image with <code>D_x</code> and <code>D_y</code> to get partials, then compute gradient magnitude
        and threshold to produce a binary edge map. Below are the outputs (smaller images for compact display).
      </p>
      <div class="grid" style="grid-template-columns:repeat(auto-fit,minmax(160px,1fr));">
        <figure><img src="proj2/results/10_plain_Ix.png" alt="Ix" style="max-width:180px"><figcaption>∂I/∂x</figcaption></figure>
        <figure><img src="proj2/results/11_plain_Iy.png" alt="Iy" style="max-width:180px"><figcaption>∂I/∂y</figcaption></figure>
        <figure><img src="proj2/results/12_plain_gradmag.png" alt="Grad mag" style="max-width:180px"><figcaption>Gradient magnitude</figcaption></figure>
        <figure><img src="proj2/results/13_plain_edges.png" alt="Edges" style="max-width:180px"><figcaption>Binary edges (thresholded)</figcaption></figure>
      </div>
    
      <h3>Part 1.3: Derivative of Gaussian (DoG)</h3>
      <p>
        To reduce noise, I first blur with a Gaussian then apply finite differences; edges become cleaner and more continuous.
        Equivalently, I precompute 2D DoG filters by convolving a Gaussian with <code>D_x</code> and <code>D_y</code> and
        apply them in one pass. The blurred-first and DoG approaches match as expected. The edges here are much more defined
        than without blur.
      </p>
      <div class="grid" style="grid-template-columns:repeat(auto-fit,minmax(160px,1fr));">
        <figure><img src="proj2/results/20_blur_img.png" alt="Blurred" style="max-width:180px"><figcaption>Gaussian-blurred image</figcaption></figure>
        <figure><img src="proj2/results/21_blur_Ix.png" alt="Blur Ix" style="max-width:180px"><figcaption>∂(G*I)/∂x</figcaption></figure>
        <figure><img src="proj2/results/22_blur_Iy.png" alt="Blur Iy" style="max-width:180px"><figcaption>∂(G*I)/∂y</figcaption></figure>
        <figure><img src="proj2/results/23_blur_gradmag.png" alt="Blur gradmag" style="max-width:180px"><figcaption>Grad. magnitude (blurred)</figcaption></figure>
      </div>
    
      <details>
        <summary>Bells &amp; Whistles: Gradient Orientation (HSV)</summary>
        <p>
          I map the gradient orientation to hue (0–π → 0–180° in OpenCV HSV), magnitude to value, and keep saturation high for visibility.
          This gives an intuitive color wheel view of edge directions.
        </p>
        <figure style="max-width:360px">
          <img src="proj2/results/orientation_hsv.png" alt="Orientation HSV">
          <figcaption>Gradient orientations visualized in HSV</figcaption>
        </figure>
      </details>
    </section>
  </div>

  <script>
    function switchTab(id){
      // buttons
      document.querySelectorAll('.tab').forEach(b=>{
        b.classList.toggle('active', b.id==='tab-'+id);
        b.setAttribute('aria-selected', b.id==='tab-'+id ? 'true':'false');
      });
      // panels
      document.querySelectorAll('[role="tabpanel"]').forEach(p=>{
        p.classList.toggle('active', p.id===id);
      });
      // hash
      history.replaceState(null, '', '#'+id);
    }
    // open tab from URL hash if present
    window.addEventListener('DOMContentLoaded', ()=>{
      const hash = (location.hash||'').replace('#','');
      if(hash==='p1') switchTab('p1');
    });
  </script>
</body>

</html>